# AWS MSK Serverless Implementation Guide

## ğŸ“‹ ëª©ì°¨

1. [ê°œìš”](#ê°œìš”)
2. [MSK Serverless ì´í•´í•˜ê¸°](#msk-serverless-ì´í•´í•˜ê¸°)
3. [ì•„í‚¤í…ì²˜ ì„¤ê³„](#ì•„í‚¤í…ì²˜-ì„¤ê³„)
4. [í´ëŸ¬ìŠ¤í„° ìƒì„± ë° ì„¤ì •](#í´ëŸ¬ìŠ¤í„°-ìƒì„±-ë°-ì„¤ì •)
5. [ë³´ì•ˆ ë° ì¸ì¦](#ë³´ì•ˆ-ë°-ì¸ì¦)
6. [Producer êµ¬í˜„](#producer-êµ¬í˜„)
7. [Consumer êµ¬í˜„](#consumer-êµ¬í˜„)
8. [í† í”½ ê´€ë¦¬](#í† í”½-ê´€ë¦¬)
9. [ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…](#ëª¨ë‹ˆí„°ë§-ë°-ë¡œê¹…)
10. [ë¹„ìš© ìµœì í™”](#ë¹„ìš©-ìµœì í™”)
11. [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)
12. [ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤](#ë² ìŠ¤íŠ¸-í”„ë™í‹°ìŠ¤)

---

## ê°œìš”

ë³¸ ë¬¸ì„œëŠ” **AWS MSK Serverless**ë¥¼ ì‚¬ìš©í•œ Kafka êµ¬í˜„ ê°€ì´ë“œì…ë‹ˆë‹¤.
MSK ServerlessëŠ” ì„œë²„ ê´€ë¦¬ ì—†ì´ Apache Kafkaë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì™„ì „ ê´€ë¦¬í˜• ì„œë¹„ìŠ¤ë¡œ,
ìš©ëŸ‰ ê³„íš ì—†ì´ ìë™ìœ¼ë¡œ í™•ì¥/ì¶•ì†Œë©ë‹ˆë‹¤.

### MSK Serverless ì„ íƒ ì´ìœ 

| íŠ¹ì§• | ì„¤ëª… |
|------|------|
| **ìë™ í™•ì¥** | íŠ¸ë˜í”½ì— ë”°ë¼ ìë™ìœ¼ë¡œ ìš©ëŸ‰ ì¡°ì • |
| **ì„œë²„ ê´€ë¦¬ ë¶ˆí•„ìš”** | ë¸Œë¡œì»¤, Zookeeper ê´€ë¦¬ ë¶ˆí•„ìš” |
| **ë¹„ìš© íš¨ìœ¨** | ì‚¬ìš©í•œ ë§Œí¼ë§Œ ì§€ë¶ˆ (Pay-as-you-go) |
| **ê³ ê°€ìš©ì„±** | ë©€í‹° AZ ë°°í¬ë¡œ ìë™ ì¥ì•  ë³µêµ¬ |
| **ë³´ì•ˆ** | IAM ê¸°ë°˜ ì¸ì¦ ë° VPC ê²©ë¦¬ |

### ê¸°ìˆ  ìŠ¤íƒ

| êµ¬ì„± ìš”ì†Œ | ê¸°ìˆ  |
|----------|------|
| Kafka í´ëŸ¬ìŠ¤í„° | AWS MSK Serverless |
| í´ë¼ì´ì–¸íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ | aiokafka (Python) |
| ì¸ì¦ | IAM, SASL/SCRAM |
| ëª¨ë‹ˆí„°ë§ | CloudWatch, MSK Metrics |
| ì¸í”„ë¼ ì½”ë“œ | Terraform / AWS CDK |

---

## MSK Serverless ì´í•´í•˜ê¸°

### MSK Serverless vs MSK Provisioned

| í•­ëª© | MSK Serverless | MSK Provisioned |
|------|----------------|-----------------|
| ìš©ëŸ‰ ê´€ë¦¬ | ìë™ | ìˆ˜ë™ |
| ìµœì†Œ ìš©ëŸ‰ | ì—†ìŒ | ë¸Œë¡œì»¤ ì¸ìŠ¤í„´ìŠ¤ í•„ìš” |
| í™•ì¥ì„± | ìë™ | ìˆ˜ë™ ì¡°ì • |
| ë¹„ìš© ëª¨ë¸ | ì‚¬ìš©ëŸ‰ ê¸°ë°˜ | ì¸ìŠ¤í„´ìŠ¤ ê¸°ë°˜ |
| ì„¤ì • ì œì–´ | ì œí•œì  | ì™„ì „í•œ ì œì–´ |
| ì‚¬ìš© ì‚¬ë¡€ | ê°€ë³€ íŠ¸ë˜í”½, ê°œë°œ/í…ŒìŠ¤íŠ¸ | ê³ ì • íŠ¸ë˜í”½, í”„ë¡œë•ì…˜ |

### MSK Serverless ì œí•œì‚¬í•­

1. **í† í”½ë‹¹ ìµœëŒ€ íŒŒí‹°ì…˜ ìˆ˜**: 1,000ê°œ
2. **í´ë¼ì´ì–¸íŠ¸ë‹¹ ìµœëŒ€ ì—°ê²° ìˆ˜**: 1,000ê°œ
3. **ë©”ì‹œì§€ í¬ê¸°**: ìµœëŒ€ 1MB
4. **ë¦¬ì „**: íŠ¹ì • ë¦¬ì „ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥
5. **Kafka ë²„ì „**: AWSê°€ ê´€ë¦¬í•˜ëŠ” íŠ¹ì • ë²„ì „ë§Œ ì§€ì›

### MSK Serverless ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Application Services                      â”‚
â”‚  (EC2, ECS, Lambda, EKS ë“±)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚ IAM / SASL ì¸ì¦
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AWS MSK Serverless Cluster                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Broker 1   â”‚  â”‚   Broker 2   â”‚  â”‚   Broker 3   â”‚     â”‚
â”‚  â”‚   (AZ-1)     â”‚  â”‚   (AZ-2)     â”‚  â”‚   (AZ-3)     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚         Auto-scaling Engine                  â”‚           â”‚
â”‚  â”‚  (íŠ¸ë˜í”½ì— ë”°ë¼ ìë™ í™•ì¥/ì¶•ì†Œ)                  â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CloudWatch Metrics & Logs                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ì•„í‚¤í…ì²˜ ì„¤ê³„

### ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜

MSK ServerlessëŠ” VPC ë‚´ì—ì„œë§Œ ë™ì‘í•©ë‹ˆë‹¤. ë‹¤ìŒ ë„¤íŠ¸ì›Œí¬ êµ¬ì„±ì´ í•„ìš”í•©ë‹ˆë‹¤:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         VPC                                  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Private Subnets                         â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚  â”‚
â”‚  â”‚  â”‚  App Server  â”‚  â”‚  App Server  â”‚                â”‚  â”‚
â”‚  â”‚  â”‚   (AZ-1)     â”‚  â”‚   (AZ-2)     â”‚                â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚            â”‚                 â”‚                              â”‚
â”‚            â–¼                 â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         MSK Serverless Cluster                       â”‚  â”‚
â”‚  â”‚  (VPC Endpointsë¥¼ í†µí•´ ì ‘ê·¼)                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         VPC Endpoints (com.amazonaws.region.kafka)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### í† í”½ ì„¤ê³„ ì „ëµ

#### ë„¤ì´ë° ì»¨ë²¤ì…˜

```
{domain}.{entity}.{event-type}

ì˜ˆì‹œ:
- order.order.created
- order.order.status-changed
- payment.payment.completed
- payment.payment.failed
- inventory.stock.updated
- user.user.registered
```

#### íŒŒí‹°ì…˜ ì „ëµ

```python
# íŒŒí‹°ì…˜ ìˆ˜ ê²°ì • ê°€ì´ë“œ
def calculate_partitions(expected_throughput: int, target_throughput_per_partition: int = 1000) -> int:
    """
    íŒŒí‹°ì…˜ ìˆ˜ ê³„ì‚°
    
    Args:
        expected_throughput: ì˜ˆìƒ ì´ˆë‹¹ ë©”ì‹œì§€ ìˆ˜
        target_throughput_per_partition: íŒŒí‹°ì…˜ë‹¹ ëª©í‘œ ì²˜ë¦¬ëŸ‰
    
    Returns:
        ê¶Œì¥ íŒŒí‹°ì…˜ ìˆ˜
    """
    partitions = (expected_throughput // target_throughput_per_partition) + 1
    # MSK Serverless ì œí•œ: ìµœëŒ€ 1,000ê°œ
    return min(partitions, 1000)

# ì˜ˆì‹œ
# ì´ˆë‹¹ 5,000 ë©”ì‹œì§€ ì˜ˆìƒ
partitions = calculate_partitions(5000, 1000)  # 6ê°œ íŒŒí‹°ì…˜
```

#### ë ˆí”Œë¦¬ì¼€ì´ì…˜ íŒ©í„°

MSK ServerlessëŠ” ìë™ìœ¼ë¡œ 3ê°œì˜ ë³µì œë³¸ì„ ìœ ì§€í•©ë‹ˆë‹¤.

### Consumer Group ì„¤ê³„

```python
# Consumer Group ë„¤ì´ë° ì»¨ë²¤ì…˜
# {service-name}.{topic-pattern}

ì˜ˆì‹œ:
- order-service.order.order.created
- notification-service.order.order.*
- analytics-service.*.*.*
```

---

## í´ëŸ¬ìŠ¤í„° ìƒì„± ë° ì„¤ì •

### 1. AWS CLIë¥¼ ì‚¬ìš©í•œ í´ëŸ¬ìŠ¤í„° ìƒì„±

#### 1.1 í´ëŸ¬ìŠ¤í„° ìƒì„±

```bash
# í´ëŸ¬ìŠ¤í„° ìƒì„±
aws kafka create-cluster \
  --cluster-name "my-msk-serverless-cluster" \
  --serverless \
  --region ap-northeast-2

# í´ëŸ¬ìŠ¤í„° ARN ì €ì¥
CLUSTER_ARN=$(aws kafka list-clusters \
  --cluster-name-filter "my-msk-serverless-cluster" \
  --query 'ClusterInfoList[0].ClusterArn' \
  --output text \
  --region ap-northeast-2)

echo "Cluster ARN: $CLUSTER_ARN"
```

#### 1.2 VPC ë° ë³´ì•ˆ ê·¸ë£¹ ì„¤ì •

```bash
# VPC ID í™•ì¸
VPC_ID="vpc-xxxxxxxxx"

# ì„œë¸Œë„· ID í™•ì¸ (ìµœì†Œ 2ê°œ AZ í•„ìš”)
SUBNET_1="subnet-xxxxxxxxx"  # AZ-1
SUBNET_2="subnet-yyyyyyyyy"  # AZ-2
SUBNET_3="subnet-zzzzzzzzz"  # AZ-3

# ë³´ì•ˆ ê·¸ë£¹ ìƒì„± (MSKìš©)
SG_ID=$(aws ec2 create-security-group \
  --group-name msk-serverless-sg \
  --description "Security group for MSK Serverless" \
  --vpc-id $VPC_ID \
  --query 'GroupId' \
  --output text)

# ì¸ë°”ìš´ë“œ ê·œì¹™ ì¶”ê°€ (ê°™ì€ VPC ë‚´ì—ì„œë§Œ ì ‘ê·¼)
aws ec2 authorize-security-group-ingress \
  --group-id $SG_ID \
  --protocol tcp \
  --port 9098 \
  --source-group $SG_ID
```

#### 1.3 í´ëŸ¬ìŠ¤í„° êµ¬ì„± ì—…ë°ì´íŠ¸

```bash
# í´ëŸ¬ìŠ¤í„°ì— VPC ë° ë³´ì•ˆ ê·¸ë£¹ ì—°ê²°
aws kafka update-cluster-configuration \
  --cluster-arn $CLUSTER_ARN \
  --current-version "1" \
  --serverless-configuration \
    ClientAuthentication="SASL_IAM" \
    VpcConfig="SubnetIds=$SUBNET_1,$SUBNET_2,$SUBNET_3,SecurityGroupIds=$SG_ID"
```

### 2. Terraformì„ ì‚¬ìš©í•œ ì¸í”„ë¼ ì½”ë“œ

#### 2.1 ê¸°ë³¸ ë¦¬ì†ŒìŠ¤

```hcl
# terraform/msk-serverless/main.tf

terraform {
  required_version = ">= 1.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "aws" {
  region = var.aws_region
}

# VPC ë°ì´í„° ì†ŒìŠ¤
data "aws_vpc" "main" {
  id = var.vpc_id
}

# ì„œë¸Œë„· ë°ì´í„° ì†ŒìŠ¤
data "aws_subnets" "private" {
  filter {
    name   = "vpc-id"
    values = [var.vpc_id]
  }
  
  tags = {
    Type = "private"
  }
}

# ë³´ì•ˆ ê·¸ë£¹
resource "aws_security_group" "msk" {
  name        = "${var.cluster_name}-sg"
  description = "Security group for MSK Serverless"
  vpc_id      = var.vpc_id

  ingress {
    description = "Kafka from VPC"
    from_port   = 9098
    to_port     = 9098
    protocol    = "tcp"
    cidr_blocks = [data.aws_vpc.main.cidr_block]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "${var.cluster_name}-sg"
  }
}

# MSK Serverless í´ëŸ¬ìŠ¤í„°
resource "aws_msk_cluster" "serverless" {
  cluster_name           = var.cluster_name
  kafka_version          = "3.5.1"  # MSK Serverless ì§€ì› ë²„ì „
  number_of_broker_nodes = 0  # ServerlessëŠ” ë¸Œë¡œì»¤ ë…¸ë“œ ìˆ˜ê°€ 0

  serverless {
    vpc_config {
      subnet_ids         = data.aws_subnets.private.ids
      security_group_ids = [aws_security_group.msk.id]
    }
  }

  tags = {
    Name        = var.cluster_name
    Environment = var.environment
  }
}

# í´ëŸ¬ìŠ¤í„° ARN ì¶œë ¥
output "cluster_arn" {
  value = aws_msk_cluster.serverless.arn
}

output "bootstrap_servers" {
  value = aws_msk_cluster.serverless.bootstrap_brokers_sasl_iam
}
```

#### 2.2 ë³€ìˆ˜ ì •ì˜

```hcl
# terraform/msk-serverless/variables.tf

variable "aws_region" {
  description = "AWS region"
  type        = string
  default     = "ap-northeast-2"
}

variable "cluster_name" {
  description = "MSK Serverless cluster name"
  type        = string
}

variable "vpc_id" {
  description = "VPC ID for MSK cluster"
  type        = string
}

variable "environment" {
  description = "Environment name"
  type        = string
  default     = "production"
}
```

### 3. AWS CDKë¥¼ ì‚¬ìš©í•œ ì¸í”„ë¼ ì½”ë“œ

```python
# infrastructure/msk_stack.py

from aws_cdk import (
    Stack,
    aws_ec2 as ec2,
    aws_msk as msk,
    CfnOutput,
)
from constructs import Construct


class MSKServerlessStack(Stack):
    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        super().__init__(scope, construct_id, **kwargs)

        # VPC ê°€ì ¸ì˜¤ê¸°
        vpc = ec2.Vpc.from_lookup(
            self, "VPC",
            vpc_id=self.node.try_get_context("vpc_id")
        )

        # ë³´ì•ˆ ê·¸ë£¹
        security_group = ec2.SecurityGroup(
            self, "MSKSecurityGroup",
            vpc=vpc,
            description="Security group for MSK Serverless",
            allow_all_outbound=True
        )

        security_group.add_ingress_rule(
            peer=ec2.Peer.ipv4(vpc.vpc_cidr_block),
            connection=ec2.Port.tcp(9098),
            description="Kafka from VPC"
        )

        # MSK Serverless í´ëŸ¬ìŠ¤í„°
        cluster = msk.CfnCluster(
            self, "MSKServerlessCluster",
            cluster_name="my-msk-serverless-cluster",
            kafka_version="3.5.1",
            number_of_broker_nodes=0,  # ServerlessëŠ” 0

            serverless={
                "vpcConfig": {
                    "subnetIds": [subnet.subnet_id for subnet in vpc.private_subnets],
                    "securityGroupIds": [security_group.security_group_id]
                }
            },

            client_authentication={
                "sasl": {
                    "iam": {
                        "enabled": True
                    }
                }
            }
        )

        # ì¶œë ¥
        CfnOutput(
            self, "ClusterArn",
            value=cluster.ref,
            description="MSK Serverless Cluster ARN"
        )

        CfnOutput(
            self, "BootstrapServers",
            value=cluster.attr_bootstrap_broker_string_sasl_iam,
            description="MSK Bootstrap Servers"
        )
```

---

## ë³´ì•ˆ ë° ì¸ì¦

### 1. IAM ì¸ì¦ ì„¤ì •

MSK ServerlessëŠ” IAM ê¸°ë°˜ ì¸ì¦ì„ ì§€ì›í•©ë‹ˆë‹¤.

#### 1.1 IAM ì •ì±… ìƒì„±

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "kafka-cluster:Connect",
        "kafka-cluster:AlterCluster",
        "kafka-cluster:DescribeCluster"
      ],
      "Resource": "arn:aws:kafka:ap-northeast-2:123456789012:cluster/my-msk-serverless-cluster/*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "kafka-cluster:*Topic*",
        "kafka-cluster:WriteData",
        "kafka-cluster:ReadData"
      ],
      "Resource": "arn:aws:kafka:ap-northeast-2:123456789012:topic/my-msk-serverless-cluster/*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "kafka-cluster:AlterGroup",
        "kafka-cluster:DescribeGroup"
      ],
      "Resource": "arn:aws:kafka:ap-northeast-2:123456789012:group/my-msk-serverless-cluster/*"
    }
  ]
}
```

#### 1.2 IAM ì—­í•  ìƒì„± (EC2/ECS/EKSìš©)

```bash
# IAM ì—­í•  ìƒì„±
aws iam create-role \
  --role-name msk-producer-role \
  --assume-role-policy-document '{
    "Version": "2012-10-17",
    "Statement": [{
      "Effect": "Allow",
      "Principal": {
        "Service": "ec2.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }]
  }'

# ì •ì±… ì—°ê²°
aws iam put-role-policy \
  --role-name msk-producer-role \
  --policy-name MSKProducerPolicy \
  --policy-document file://msk-policy.json
```

### 2. Python í´ë¼ì´ì–¸íŠ¸ì—ì„œ IAM ì¸ì¦ ì‚¬ìš©

#### 2.1 boto3ë¥¼ ì‚¬ìš©í•œ ì¸ì¦

```python
# src/infrastructure/messaging/msk_auth.py

import boto3
from aiokafka import AIOKafkaProducer, AIOKafkaConsumer
from aiokafka.helpers import create_ssl_context
import ssl

class MSKIAMAuth:
    """MSK IAM ì¸ì¦ í—¬í¼"""
    
    def __init__(self, region: str = "ap-northeast-2"):
        self.region = region
        self.session = boto3.Session()
    
    def get_sasl_mechanism(self) -> str:
        """SASL ë©”ì»¤ë‹ˆì¦˜ ë°˜í™˜"""
        return "AWS_MSK_IAM"
    
    def get_sasl_plain_username(self) -> str:
        """IAM ì‚¬ìš©ìëª… (ë¹ˆ ë¬¸ìì—´)"""
        return ""
    
    def get_sasl_plain_password(self) -> str:
        """IAM ì¸ì¦ í† í° ìƒì„±"""
        # boto3ë¥¼ ì‚¬ìš©í•˜ì—¬ MSK ì¸ì¦ í† í° ìƒì„±
        # ì‹¤ì œë¡œëŠ” aiokafkaê°€ ë‚´ë¶€ì ìœ¼ë¡œ ì²˜ë¦¬
        return ""
    
    def get_security_protocol(self) -> str:
        """ë³´ì•ˆ í”„ë¡œí† ì½œ"""
        return "SASL_SSL"
    
    def get_ssl_context(self) -> ssl.SSLContext:
        """SSL ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        context = create_ssl_context()
        context.check_hostname = False
        return context
```

#### 2.2 í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# .env íŒŒì¼
KAFKA_BOOTSTRAP_SERVERS=b-1.my-msk-cluster.xxxxx.c2.kafka-serverless.ap-northeast-2.amazonaws.com:9098
KAFKA_REGION=ap-northeast-2
KAFKA_SECURITY_PROTOCOL=SASL_SSL
KAFKA_SASL_MECHANISM=AWS_MSK_IAM
```

### 3. VPC ì—”ë“œí¬ì¸íŠ¸ ì„¤ì • (ì„ íƒì‚¬í•­)

í”„ë¼ì´ë¹— ì„œë¸Œë„·ì—ì„œ ì¸í„°ë„· ê²Œì´íŠ¸ì›¨ì´ ì—†ì´ MSKì— ì ‘ê·¼í•˜ë ¤ë©´ VPC ì—”ë“œí¬ì¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.

```hcl
# VPC ì—”ë“œí¬ì¸íŠ¸
resource "aws_vpc_endpoint" "kafka" {
  vpc_id              = var.vpc_id
  service_name         = "com.amazonaws.${var.aws_region}.kafka"
  vpc_endpoint_type    = "Interface"
  subnet_ids           = data.aws_subnets.private.ids
  security_group_ids   = [aws_security_group.vpc_endpoint.id]
  private_dns_enabled  = true

  tags = {
    Name = "msk-vpc-endpoint"
  }
}
```

---

## Producer êµ¬í˜„

### 1. ê¸°ë³¸ Producer êµ¬í˜„

```python
# src/infrastructure/messaging/producer.py

import json
import asyncio
from typing import Optional, Dict, Any
from datetime import datetime
import uuid

from aiokafka import AIOKafkaProducer
import boto3
from botocore.auth import SigV4Auth
from botocore.awsrequest import AWSRequest

from src.core.config import settings
from src.core.logging import logger
from src.domain.events.base import DomainEvent


class MSKProducer:
    """MSK Serverless Producer with IAM authentication"""
    
    def __init__(
        self,
        bootstrap_servers: str,
        region: str = "ap-northeast-2",
    ):
        self.bootstrap_servers = bootstrap_servers
        self.region = region
        self.producer: Optional[AIOKafkaProducer] = None
        self._session = boto3.Session(region_name=region)
    
    async def start(self) -> None:
        """Producer ì‹œì‘"""
        logger.info(
            "Starting MSK Producer...",
            bootstrap_servers=self.bootstrap_servers,
            region=self.region
        )
        
        # IAM ì¸ì¦ì„ ìœ„í•œ SASL ì„¤ì •
        self.producer = AIOKafkaProducer(
            bootstrap_servers=self.bootstrap_servers,
            security_protocol="SASL_SSL",
            sasl_mechanism="AWS_MSK_IAM",
            sasl_plain_username="",  # IAM ì¸ì¦ì—ì„œëŠ” ë¹ˆ ë¬¸ìì—´
            sasl_plain_password="",  # aiokafkaê°€ ë‚´ë¶€ì ìœ¼ë¡œ ì²˜ë¦¬
            ssl_context=self._create_ssl_context(),
            # Producer ì„¤ì •
            acks="all",  # ëª¨ë“  ë³µì œë³¸ì— ì“°ê¸° ì™„ë£Œ í›„ ì‘ë‹µ
            enable_idempotence=True,  # ë©±ë“±ì„± í™œì„±í™”
            max_batch_size=16384,  # 16KB
            linger_ms=10,  # ë°°ì¹˜ ëŒ€ê¸° ì‹œê°„
            compression_type="gzip",  # ì••ì¶•
            retries=3,  # ì¬ì‹œë„ íšŸìˆ˜
            request_timeout_ms=30000,  # ìš”ì²­ íƒ€ì„ì•„ì›ƒ
            client_id=f"producer-{uuid.uuid4().hex[:8]}",
        )
        
        await self.producer.start()
        logger.info("MSK Producer started")
    
    async def stop(self) -> None:
        """Producer ì¢…ë£Œ"""
        if self.producer:
            logger.info("Stopping MSK Producer...")
            await self.producer.stop()
            self.producer = None
            logger.info("MSK Producer stopped")
    
    def _create_ssl_context(self):
        """SSL ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        import ssl
        from aiokafka.helpers import create_ssl_context
        
        context = create_ssl_context()
        context.check_hostname = False
        return context
    
    async def publish(
        self,
        topic: str,
        key: str,
        value: Dict[str, Any],
        headers: Optional[Dict[str, str]] = None,
    ) -> None:
        """
        ë©”ì‹œì§€ ë°œí–‰
        
        Args:
            topic: í† í”½ ì´ë¦„
            key: íŒŒí‹°ì…˜ í‚¤
            value: ë©”ì‹œì§€ ê°’
            headers: ë©”ì‹œì§€ í—¤ë”
        """
        if not self.producer:
            raise RuntimeError("Producer not started. Call start() first.")
        
        try:
            # ë©”ì‹œì§€ í—¤ë” ì¶”ê°€
            kafka_headers = []
            if headers:
                for k, v in headers.items():
                    kafka_headers.append((k, v.encode("utf-8")))
            
            # ë©”ì‹œì§€ ë°œí–‰
            future = await self.producer.send(
                topic=topic,
                key=key.encode("utf-8") if key else None,
                value=json.dumps(value).encode("utf-8"),
                headers=kafka_headers,
            )
            
            # ì‘ë‹µ ëŒ€ê¸°
            record_metadata = await future
            
            logger.info(
                "Message published",
                topic=topic,
                partition=record_metadata.partition,
                offset=record_metadata.offset,
                key=key,
            )
            
        except Exception as e:
            logger.error(
                "Failed to publish message",
                topic=topic,
                key=key,
                error=str(e),
                exc_info=True
            )
            raise
    
    async def publish_event(self, event: DomainEvent) -> None:
        """
        ë„ë©”ì¸ ì´ë²¤íŠ¸ ë°œí–‰
        
        Args:
            event: ë„ë©”ì¸ ì´ë²¤íŠ¸
        """
        await self.publish(
            topic=event.topic,
            key=event.partition_key,
            value=event.to_dict(),
            headers={
                "event_type": event.event_type,
                "event_id": str(event.event_id),
                "timestamp": datetime.utcnow().isoformat(),
            }
        )
```

### 2. Producer íŒ©í† ë¦¬ ë° ì‹±ê¸€í†¤

```python
# src/infrastructure/messaging/producer_factory.py

from typing import Optional
from src.infrastructure.messaging.producer import MSKProducer
from src.core.config import settings
from src.core.logging import logger

_producer_instance: Optional[MSKProducer] = None


async def get_producer() -> MSKProducer:
    """Producer ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _producer_instance
    
    if _producer_instance is None:
        _producer_instance = MSKProducer(
            bootstrap_servers=settings.KAFKA_BOOTSTRAP_SERVERS,
            region=settings.KAFKA_REGION,
        )
        await _producer_instance.start()
    
    return _producer_instance


async def close_producer() -> None:
    """Producer ì¢…ë£Œ"""
    global _producer_instance
    
    if _producer_instance:
        await _producer_instance.stop()
        _producer_instance = None
        logger.info("Producer closed")
```

### 3. ì´ë²¤íŠ¸ ë°œí–‰ ì˜ˆì‹œ

```python
# src/domain/events/order_created.py

from dataclasses import dataclass
from datetime import datetime
from uuid import UUID, uuid4
from typing import Dict, Any

from src.domain.events.base import DomainEvent


@dataclass
class OrderCreatedEvent(DomainEvent):
    """ì£¼ë¬¸ ìƒì„± ì´ë²¤íŠ¸"""
    
    order_id: str
    user_id: str
    total_amount: int
    items: list[Dict[str, Any]]
    
    @property
    def topic(self) -> str:
        return "order.order.created"
    
    @property
    def event_type(self) -> str:
        return "order.created"
    
    @property
    def partition_key(self) -> str:
        return self.order_id
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "event_id": str(self.event_id),
            "event_type": self.event_type,
            "timestamp": self.timestamp.isoformat(),
            "order_id": self.order_id,
            "user_id": self.user_id,
            "total_amount": self.total_amount,
            "items": self.items,
        }


# ì‚¬ìš© ì˜ˆì‹œ
async def create_order(order_data: Dict[str, Any]) -> None:
    # ì£¼ë¬¸ ìƒì„± ë¡œì§...
    
    # ì´ë²¤íŠ¸ ë°œí–‰
    event = OrderCreatedEvent(
        event_id=uuid4(),
        timestamp=datetime.utcnow(),
        order_id=order_data["order_id"],
        user_id=order_data["user_id"],
        total_amount=order_data["total_amount"],
        items=order_data["items"],
    )
    
    producer = await get_producer()
    await producer.publish_event(event)
```

### 4. ë°°ì¹˜ ë°œí–‰ (ì„±ëŠ¥ ìµœì í™”)

```python
# src/infrastructure/messaging/batch_producer.py

from typing import List, Tuple
from src.infrastructure.messaging.producer import MSKProducer


class BatchProducer:
    """ë°°ì¹˜ ë©”ì‹œì§€ ë°œí–‰"""
    
    def __init__(self, producer: MSKProducer, batch_size: int = 100):
        self.producer = producer
        self.batch_size = batch_size
        self.buffer: List[Tuple[str, str, dict]] = []
    
    async def add(self, topic: str, key: str, value: dict) -> None:
        """ë©”ì‹œì§€ ë²„í¼ì— ì¶”ê°€"""
        self.buffer.append((topic, key, value))
        
        if len(self.buffer) >= self.batch_size:
            await self.flush()
    
    async def flush(self) -> None:
        """ë²„í¼ì˜ ëª¨ë“  ë©”ì‹œì§€ ë°œí–‰"""
        if not self.buffer:
            return
        
        # ë³‘ë ¬ ë°œí–‰
        tasks = [
            self.producer.publish(topic, key, value)
            for topic, key, value in self.buffer
        ]
        
        await asyncio.gather(*tasks, return_exceptions=True)
        
        logger.info(f"Flushed {len(self.buffer)} messages")
        self.buffer.clear()
```

---

## Consumer êµ¬í˜„

### 1. ê¸°ë³¸ Consumer êµ¬í˜„

```python
# src/infrastructure/messaging/consumer.py

import asyncio
import json
from typing import List, Optional, Callable, Awaitable, Dict, Any
from uuid import uuid4

from aiokafka import AIOKafkaConsumer
from aiokafka.errors import KafkaError

from src.core.config import settings
from src.core.logging import logger


EventHandler = Callable[[Dict[str, Any]], Awaitable[None]]


class MSKConsumer:
    """MSK Serverless Consumer with IAM authentication"""
    
    def __init__(
        self,
        topics: List[str],
        group_id: str,
        bootstrap_servers: str,
        region: str = "ap-northeast-2",
    ):
        self.topics = topics
        self.group_id = group_id
        self.bootstrap_servers = bootstrap_servers
        self.region = region
        self.consumer: Optional[AIOKafkaConsumer] = None
        self.handlers: Dict[str, EventHandler] = {}
        self.running = False
    
    def register_handler(self, event_type: str, handler: EventHandler) -> None:
        """
        ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ë“±ë¡
        
        Args:
            event_type: ì´ë²¤íŠ¸ íƒ€ì… (ì˜ˆ: "order.created")
            handler: ì´ë²¤íŠ¸ ì²˜ë¦¬ í•¨ìˆ˜
        """
        self.handlers[event_type] = handler
        logger.info(f"Registered handler for event type: {event_type}")
    
    async def start(self) -> None:
        """Consumer ì‹œì‘"""
        logger.info(
            "Starting MSK Consumer...",
            topics=self.topics,
            group_id=self.group_id,
            bootstrap_servers=self.bootstrap_servers
        )
        
        self.consumer = AIOKafkaConsumer(
            *self.topics,
            bootstrap_servers=self.bootstrap_servers,
            group_id=self.group_id,
            security_protocol="SASL_SSL",
            sasl_mechanism="AWS_MSK_IAM",
            sasl_plain_username="",
            sasl_plain_password="",
            ssl_context=self._create_ssl_context(),
            # Consumer ì„¤ì •
            auto_offset_reset="earliest",  # ë˜ëŠ” "latest"
            enable_auto_commit=False,  # ìˆ˜ë™ ì»¤ë°‹
            max_poll_records=100,  # í•œ ë²ˆì— ê°€ì ¸ì˜¬ ìµœëŒ€ ë ˆì½”ë“œ ìˆ˜
            session_timeout_ms=30000,  # ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ
            heartbeat_interval_ms=3000,  # í•˜íŠ¸ë¹„íŠ¸ ê°„ê²©
            value_deserializer=lambda m: json.loads(m.decode("utf-8")),
            key_deserializer=lambda k: k.decode("utf-8") if k else None,
            client_id=f"consumer-{uuid4().hex[:8]}",
        )
        
        await self.consumer.start()
        self.running = True
        
        logger.info("MSK Consumer started")
        
        # ë©”ì‹œì§€ ì†Œë¹„ ë£¨í”„ ì‹œì‘
        asyncio.create_task(self._consume_loop())
    
    async def stop(self) -> None:
        """Consumer ì¢…ë£Œ"""
        logger.info("Stopping MSK Consumer...")
        self.running = False
        
        if self.consumer:
            await self.consumer.stop()
            self.consumer = None
        
        logger.info("MSK Consumer stopped")
    
    def _create_ssl_context(self):
        """SSL ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        import ssl
        from aiokafka.helpers import create_ssl_context
        
        context = create_ssl_context()
        context.check_hostname = False
        return context
    
    async def _consume_loop(self) -> None:
        """ë©”ì‹œì§€ ì†Œë¹„ ë£¨í”„"""
        try:
            while self.running:
                try:
                    # ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
                    msg_pack = await asyncio.wait_for(
                        self.consumer.getmany(timeout_ms=1000, max_records=100),
                        timeout=2.0
                    )
                    
                    for topic_partition, messages in msg_pack.items():
                        for message in messages:
                            await self._process_message(message)
                    
                    # ìˆ˜ë™ ì»¤ë°‹
                    await self.consumer.commit()
                    
                except asyncio.TimeoutError:
                    # íƒ€ì„ì•„ì›ƒì€ ì •ìƒ (ë©”ì‹œì§€ê°€ ì—†ì„ ë•Œ)
                    continue
                except KafkaError as e:
                    logger.error(f"Kafka error: {e}", exc_info=True)
                    await asyncio.sleep(1)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                except Exception as e:
                    logger.error(f"Unexpected error in consume loop: {e}", exc_info=True)
                    await asyncio.sleep(1)
        
        except asyncio.CancelledError:
            logger.info("Consume loop cancelled")
        except Exception as e:
            logger.error(f"Fatal error in consume loop: {e}", exc_info=True)
            self.running = False
    
    async def _process_message(self, message) -> None:
        """ë©”ì‹œì§€ ì²˜ë¦¬"""
        try:
            event_data = message.value
            event_type = event_data.get("event_type")
            
            logger.debug(
                "Processing message",
                topic=message.topic,
                partition=message.partition,
                offset=message.offset,
                event_type=event_type,
            )
            
            # í•¸ë“¤ëŸ¬ ì°¾ê¸°
            handler = self.handlers.get(event_type)
            
            if handler:
                # í•¸ë“¤ëŸ¬ ì‹¤í–‰
                await handler(event_data)
                
                logger.info(
                    "Message processed successfully",
                    topic=message.topic,
                    partition=message.partition,
                    offset=message.offset,
                    event_type=event_type,
                )
            else:
                logger.warning(
                    "No handler for event type",
                    event_type=event_type,
                    topic=message.topic,
                )
        
        except Exception as e:
            logger.error(
                "Error processing message",
                topic=message.topic,
                partition=message.partition,
                offset=message.offset,
                error=str(e),
                exc_info=True
            )
            # Dead Letter Queueë¡œ ì „ì†¡í•˜ê±°ë‚˜ ì¬ì‹œë„ ë¡œì§ êµ¬í˜„
            raise
```

### 2. Consumer ê·¸ë£¹ ê´€ë¦¬

```python
# src/infrastructure/messaging/consumer_manager.py

from typing import List, Dict
from src.infrastructure.messaging.consumer import MSKConsumer, EventHandler
from src.core.config import settings
from src.core.logging import logger


class ConsumerManager:
    """ì—¬ëŸ¬ Consumer ê´€ë¦¬"""
    
    def __init__(self):
        self.consumers: List[MSKConsumer] = []
    
    def create_consumer(
        self,
        topics: List[str],
        group_id: str,
        handlers: Dict[str, EventHandler],
    ) -> MSKConsumer:
        """Consumer ìƒì„± ë° ë“±ë¡"""
        consumer = MSKConsumer(
            topics=topics,
            group_id=group_id,
            bootstrap_servers=settings.KAFKA_BOOTSTRAP_SERVERS,
            region=settings.KAFKA_REGION,
        )
        
        # í•¸ë“¤ëŸ¬ ë“±ë¡
        for event_type, handler in handlers.items():
            consumer.register_handler(event_type, handler)
        
        self.consumers.append(consumer)
        return consumer
    
    async def start_all(self) -> None:
        """ëª¨ë“  Consumer ì‹œì‘"""
        logger.info(f"Starting {len(self.consumers)} consumers...")
        
        tasks = [consumer.start() for consumer in self.consumers]
        await asyncio.gather(*tasks)
        
        logger.info("All consumers started")
    
    async def stop_all(self) -> None:
        """ëª¨ë“  Consumer ì¢…ë£Œ"""
        logger.info(f"Stopping {len(self.consumers)} consumers...")
        
        tasks = [consumer.stop() for consumer in self.consumers]
        await asyncio.gather(*tasks)
        
        logger.info("All consumers stopped")
```

### 3. ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì˜ˆì‹œ

```python
# src/application/handlers/order_handler.py

from typing import Dict, Any
from src.core.logging import logger
from src.domain.services.inventory_service import InventoryService
from src.domain.services.notification_service import NotificationService


async def handle_order_created(event_data: Dict[str, Any]) -> None:
    """ì£¼ë¬¸ ìƒì„± ì´ë²¤íŠ¸ ì²˜ë¦¬"""
    logger.info("Handling order.created event", event_data=event_data)
    
    order_id = event_data["order_id"]
    items = event_data["items"]
    
    # ì¬ê³  ì°¨ê°
    inventory_service = InventoryService()
    for item in items:
        await inventory_service.decrease_stock(
            product_id=item["product_id"],
            quantity=item["quantity"]
        )
    
    # ì•Œë¦¼ ë°œì†¡
    notification_service = NotificationService()
    await notification_service.send_order_confirmation(
        user_id=event_data["user_id"],
        order_id=order_id
    )
    
    logger.info("Order created event processed", order_id=order_id)


async def handle_payment_completed(event_data: Dict[str, Any]) -> None:
    """ê²°ì œ ì™„ë£Œ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
    logger.info("Handling payment.completed event", event_data=event_data)
    
    order_id = event_data["order_id"]
    
    # ì£¼ë¬¸ ìƒíƒœ ì—…ë°ì´íŠ¸
    order_service = OrderService()
    await order_service.update_status(order_id, "paid")
    
    logger.info("Payment completed event processed", order_id=order_id)
```

### 4. ì• í”Œë¦¬ì¼€ì´ì…˜ í†µí•©

```python
# src/main.py

import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI

from src.infrastructure.messaging.producer_factory import get_producer, close_producer
from src.infrastructure.messaging.consumer_manager import ConsumerManager
from src.application.handlers.order_handler import (
    handle_order_created,
    handle_payment_completed,
)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    # ì‹œì‘ ì‹œ
    producer = await get_producer()
    
    # Consumer ì„¤ì •
    consumer_manager = ConsumerManager()
    
    # ì£¼ë¬¸ ì´ë²¤íŠ¸ Consumer
    consumer_manager.create_consumer(
        topics=["order.order.created", "order.order.status-changed"],
        group_id="order-service-consumer",
        handlers={
            "order.created": handle_order_created,
            "order.status-changed": handle_order_status_changed,
        }
    )
    
    # ê²°ì œ ì´ë²¤íŠ¸ Consumer
    consumer_manager.create_consumer(
        topics=["payment.payment.completed", "payment.payment.failed"],
        group_id="payment-service-consumer",
        handlers={
            "payment.completed": handle_payment_completed,
            "payment.failed": handle_payment_failed,
        }
    )
    
    await consumer_manager.start_all()
    
    yield
    
    # ì¢…ë£Œ ì‹œ
    await consumer_manager.stop_all()
    await close_producer()


app = FastAPI(lifespan=lifespan)
```

---

## í† í”½ ê´€ë¦¬

### 1. í† í”½ ìƒì„± (AWS CLI)

```bash
# í† í”½ ìƒì„±
aws kafka create-topic \
  --cluster-arn $CLUSTER_ARN \
  --topic-name "order.order.created" \
  --partitions 6 \
  --replication-factor 3 \
  --region ap-northeast-2

# í† í”½ ëª©ë¡ ì¡°íšŒ
aws kafka list-topics \
  --cluster-arn $CLUSTER_ARN \
  --region ap-northeast-2

# í† í”½ ìƒì„¸ ì •ë³´
aws kafka describe-topic \
  --cluster-arn $CLUSTER_ARN \
  --topic-name "order.order.created" \
  --region ap-northeast-2
```

### 2. Pythonìœ¼ë¡œ í† í”½ ê´€ë¦¬

```python
# src/infrastructure/messaging/topic_manager.py

from typing import List, Dict, Any
from aiokafka import AIOKafkaAdminClient
from aiokafka.admin import NewTopic, ConfigResource, ConfigResourceType
from src.core.config import settings
from src.core.logging import logger


class TopicManager:
    """í† í”½ ê´€ë¦¬"""
    
    def __init__(self, bootstrap_servers: str, region: str = "ap-northeast-2"):
        self.bootstrap_servers = bootstrap_servers
        self.region = region
        self.admin_client: Optional[AIOKafkaAdminClient] = None
    
    async def start(self) -> None:
        """Admin í´ë¼ì´ì–¸íŠ¸ ì‹œì‘"""
        self.admin_client = AIOKafkaAdminClient(
            bootstrap_servers=self.bootstrap_servers,
            security_protocol="SASL_SSL",
            sasl_mechanism="AWS_MSK_IAM",
            sasl_plain_username="",
            sasl_plain_password="",
            ssl_context=self._create_ssl_context(),
        )
        await self.admin_client.start()
        logger.info("Topic manager started")
    
    async def stop(self) -> None:
        """Admin í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ"""
        if self.admin_client:
            await self.admin_client.close()
            self.admin_client = None
            logger.info("Topic manager stopped")
    
    def _create_ssl_context(self):
        """SSL ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        import ssl
        from aiokafka.helpers import create_ssl_context
        
        context = create_ssl_context()
        context.check_hostname = False
        return context
    
    async def create_topic(
        self,
        topic_name: str,
        num_partitions: int = 6,
        replication_factor: int = 3,
        config: Optional[Dict[str, str]] = None,
    ) -> None:
        """
        í† í”½ ìƒì„±
        
        Args:
            topic_name: í† í”½ ì´ë¦„
            num_partitions: íŒŒí‹°ì…˜ ìˆ˜
            replication_factor: ë³µì œ íŒ©í„° (MSK ServerlessëŠ” ìë™ ê´€ë¦¬)
            config: í† í”½ ì„¤ì •
        """
        if not self.admin_client:
            raise RuntimeError("Topic manager not started")
        
        topic = NewTopic(
            name=topic_name,
            num_partitions=num_partitions,
            replication_factor=replication_factor,
            topic_configs=config or {},
        )
        
        try:
            result = await self.admin_client.create_topics([topic])
            await result[topic_name]
            logger.info(f"Topic created: {topic_name}")
        except Exception as e:
            logger.error(f"Failed to create topic {topic_name}: {e}")
            raise
    
    async def delete_topic(self, topic_name: str) -> None:
        """í† í”½ ì‚­ì œ"""
        if not self.admin_client:
            raise RuntimeError("Topic manager not started")
        
        try:
            result = await self.admin_client.delete_topics([topic_name])
            await result[topic_name]
            logger.info(f"Topic deleted: {topic_name}")
        except Exception as e:
            logger.error(f"Failed to delete topic {topic_name}: {e}")
            raise
    
    async def list_topics(self) -> List[str]:
        """í† í”½ ëª©ë¡ ì¡°íšŒ"""
        if not self.admin_client:
            raise RuntimeError("Topic manager not started")
        
        metadata = await self.admin_client.describe_topics()
        return list(metadata.topics.keys())
    
    async def get_topic_config(self, topic_name: str) -> Dict[str, Any]:
        """í† í”½ ì„¤ì • ì¡°íšŒ"""
        if not self.admin_client:
            raise RuntimeError("Topic manager not started")
        
        config_resource = ConfigResource(
            ConfigResourceType.TOPIC,
            topic_name,
        )
        
        result = await self.admin_client.describe_configs([config_resource])
        configs = await result[config_resource]
        
        return {k: v for k, v in configs.items()}
```

### 3. í† í”½ ì„¤ì • ê¶Œì¥ì‚¬í•­

```python
# í† í”½ ì„¤ì • ì˜ˆì‹œ
topic_config = {
    # ë©”ì‹œì§€ ë³´ê´€ ê¸°ê°„ (7ì¼)
    "retention.ms": "604800000",
    
    # ì••ì¶• íƒ€ì…
    "compression.type": "gzip",
    
    # ìµœëŒ€ ë©”ì‹œì§€ í¬ê¸°
    "max.message.bytes": "1048576",  # 1MB
    
    # ì„¸ê·¸ë¨¼íŠ¸ í¬ê¸°
    "segment.bytes": "1073741824",  # 1GB
}
```

---

## ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### 1. CloudWatch ë©”íŠ¸ë¦­

MSK ServerlessëŠ” ìë™ìœ¼ë¡œ CloudWatch ë©”íŠ¸ë¦­ì„ ì œê³µí•©ë‹ˆë‹¤.

#### ì£¼ìš” ë©”íŠ¸ë¦­

| ë©”íŠ¸ë¦­ | ì„¤ëª… |
|--------|------|
| `BytesInPerSec` | ì´ˆë‹¹ ìˆ˜ì‹  ë°”ì´íŠ¸ ìˆ˜ |
| `BytesOutPerSec` | ì´ˆë‹¹ ì†¡ì‹  ë°”ì´íŠ¸ ìˆ˜ |
| `MessagesInPerSec` | ì´ˆë‹¹ ìˆ˜ì‹  ë©”ì‹œì§€ ìˆ˜ |
| `SumOffsetLag` | Consumer lag í•©ê³„ |
| `SumPartitionCount` | íŒŒí‹°ì…˜ ìˆ˜ í•©ê³„ |

#### CloudWatch ëŒ€ì‹œë³´ë“œ ìƒì„±

```python
# monitoring/cloudwatch_dashboard.py

import boto3

cloudwatch = boto3.client('cloudwatch')

dashboard_body = {
    "widgets": [
        {
            "type": "metric",
            "properties": {
                "metrics": [
                    ["AWS/Kafka", "BytesInPerSec", {"stat": "Sum"}],
                    [".", "BytesOutPerSec", {"stat": "Sum"}]
                ],
                "period": 300,
                "stat": "Average",
                "region": "ap-northeast-2",
                "title": "Kafka Throughput"
            }
        },
        {
            "type": "metric",
            "properties": {
                "metrics": [
                    ["AWS/Kafka", "MessagesInPerSec", {"stat": "Sum"}]
                ],
                "period": 300,
                "stat": "Average",
                "region": "ap-northeast-2",
                "title": "Message Rate"
            }
        },
        {
            "type": "metric",
            "properties": {
                "metrics": [
                    ["AWS/Kafka", "SumOffsetLag", {"stat": "Sum"}]
                ],
                "period": 300,
                "stat": "Average",
                "region": "ap-northeast-2",
                "title": "Consumer Lag"
            }
        }
    ]
}

cloudwatch.put_dashboard(
    DashboardName="MSK-Serverless-Dashboard",
    DashboardBody=json.dumps(dashboard_body)
)
```

### 2. ì• í”Œë¦¬ì¼€ì´ì…˜ ë ˆë²¨ ëª¨ë‹ˆí„°ë§

```python
# src/infrastructure/messaging/metrics.py

from typing import Dict
from dataclasses import dataclass
from datetime import datetime
import time

from src.core.logging import logger


@dataclass
class ProducerMetrics:
    """Producer ë©”íŠ¸ë¦­"""
    messages_sent: int = 0
    messages_failed: int = 0
    bytes_sent: int = 0
    avg_latency_ms: float = 0.0
    last_sent_at: Optional[datetime] = None


@dataclass
class ConsumerMetrics:
    """Consumer ë©”íŠ¸ë¦­"""
    messages_consumed: int = 0
    messages_failed: int = 0
    bytes_consumed: int = 0
    avg_processing_time_ms: float = 0.0
    current_lag: int = 0
    last_consumed_at: Optional[datetime] = None


class MetricsCollector:
    """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.producer_metrics: Dict[str, ProducerMetrics] = {}
        self.consumer_metrics: Dict[str, ConsumerMetrics] = {}
    
    def record_producer_success(
        self,
        topic: str,
        bytes_sent: int,
        latency_ms: float,
    ) -> None:
        """Producer ì„±ê³µ ê¸°ë¡"""
        if topic not in self.producer_metrics:
            self.producer_metrics[topic] = ProducerMetrics()
        
        metrics = self.producer_metrics[topic]
        metrics.messages_sent += 1
        metrics.bytes_sent += bytes_sent
        metrics.last_sent_at = datetime.utcnow()
        
        # í‰ê·  ì§€ì—° ì‹œê°„ ê³„ì‚°
        total = metrics.messages_sent + metrics.messages_failed
        metrics.avg_latency_ms = (
            (metrics.avg_latency_ms * (total - 1) + latency_ms) / total
        )
    
    def record_producer_failure(self, topic: str) -> None:
        """Producer ì‹¤íŒ¨ ê¸°ë¡"""
        if topic not in self.producer_metrics:
            self.producer_metrics[topic] = ProducerMetrics()
        
        self.producer_metrics[topic].messages_failed += 1
    
    def record_consumer_success(
        self,
        topic: str,
        bytes_consumed: int,
        processing_time_ms: float,
    ) -> None:
        """Consumer ì„±ê³µ ê¸°ë¡"""
        if topic not in self.consumer_metrics:
            self.consumer_metrics[topic] = ConsumerMetrics()
        
        metrics = self.consumer_metrics[topic]
        metrics.messages_consumed += 1
        metrics.bytes_consumed += bytes_consumed
        metrics.last_consumed_at = datetime.utcnow()
        
        # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        total = metrics.messages_consumed + metrics.messages_failed
        metrics.avg_processing_time_ms = (
            (metrics.avg_processing_time_ms * (total - 1) + processing_time_ms) / total
        )
    
    def record_consumer_failure(self, topic: str) -> None:
        """Consumer ì‹¤íŒ¨ ê¸°ë¡"""
        if topic not in self.consumer_metrics:
            self.consumer_metrics[topic] = ConsumerMetrics()
        
        self.consumer_metrics[topic].messages_failed += 1
    
    def get_producer_metrics(self, topic: str) -> ProducerMetrics:
        """Producer ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        return self.producer_metrics.get(topic, ProducerMetrics())
    
    def get_consumer_metrics(self, topic: str) -> ConsumerMetrics:
        """Consumer ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        return self.consumer_metrics.get(topic, ConsumerMetrics())
    
    def log_metrics(self) -> None:
        """ë©”íŠ¸ë¦­ ë¡œê¹…"""
        logger.info("Producer Metrics", metrics=self.producer_metrics)
        logger.info("Consumer Metrics", metrics=self.consumer_metrics)
```

### 3. ì•ŒëŒ ì„¤ì •

```python
# monitoring/alarms.py

import boto3

cloudwatch = boto3.client('cloudwatch')

# Consumer Lag ì•ŒëŒ
cloudwatch.put_metric_alarm(
    AlarmName='MSK-High-Consumer-Lag',
    ComparisonOperator='GreaterThanThreshold',
    EvaluationPeriods=2,
    MetricName='SumOffsetLag',
    Namespace='AWS/Kafka',
    Period=300,
    Statistic='Average',
    Threshold=10000,
    ActionsEnabled=True,
    AlarmActions=['arn:aws:sns:ap-northeast-2:123456789012:alerts'],
    AlarmDescription='Alert when consumer lag exceeds 10,000 messages'
)

# ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨ ì•ŒëŒ
cloudwatch.put_metric_alarm(
    AlarmName='MSK-High-Error-Rate',
    ComparisonOperator='GreaterThanThreshold',
    EvaluationPeriods=1,
    MetricName='MessagesInPerSec',
    Namespace='Custom/Kafka',
    Period=60,
    Statistic='Sum',
    Threshold=100,
    ActionsEnabled=True,
    AlarmActions=['arn:aws:sns:ap-northeast-2:123456789012:alerts'],
    AlarmDescription='Alert when error rate is high'
)
```

---

## ë¹„ìš© ìµœì í™”

### 1. MSK Serverless ë¹„ìš© êµ¬ì¡°

MSK ServerlessëŠ” ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ê³¼ê¸ˆë©ë‹ˆë‹¤:

- **í”„ë¡œë“€ì„œ ì“°ê¸°**: GBë‹¹ $0.10
- **ì»¨ìŠˆë¨¸ ì½ê¸°**: GBë‹¹ $0.10
- **ìŠ¤í† ë¦¬ì§€**: GB-ì›”ë‹¹ $0.10

### 2. ë¹„ìš© ìµœì í™” ì „ëµ

#### 2.1 ë©”ì‹œì§€ ì••ì¶•

```python
# Producerì—ì„œ ì••ì¶• í™œì„±í™”
producer = AIOKafkaProducer(
    # ...
    compression_type="gzip",  # ë˜ëŠ” "snappy", "lz4"
    # ...
)
```

#### 2.2 ë°°ì¹˜ ë°œí–‰

```python
# ì—¬ëŸ¬ ë©”ì‹œì§€ë¥¼ ë°°ì¹˜ë¡œ ë°œí–‰í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œ ê°ì†Œ
async def batch_publish(producer, messages: List[Tuple[str, str, dict]]):
    futures = [
        producer.send(topic, key, value)
        for topic, key, value in messages
    ]
    await asyncio.gather(*futures)
```

#### 2.3 ë©”ì‹œì§€ í¬ê¸° ìµœì í™”

```python
# ë¶ˆí•„ìš”í•œ ë°ì´í„° ì œê±°
def optimize_message(event: DomainEvent) -> dict:
    """ë©”ì‹œì§€ í¬ê¸° ìµœì í™”"""
    data = event.to_dict()
    
    # ë¶ˆí•„ìš”í•œ í•„ë“œ ì œê±°
    data.pop("internal_metadata", None)
    data.pop("debug_info", None)
    
    return data
```

#### 2.4 í† í”½ ë³´ê´€ ì •ì±…

```python
# ì˜¤ë˜ëœ ë©”ì‹œì§€ ìë™ ì‚­ì œ
topic_config = {
    "retention.ms": "604800000",  # 7ì¼
    "cleanup.policy": "delete",  # ë˜ëŠ” "compact"
}
```

### 3. ë¹„ìš© ëª¨ë‹ˆí„°ë§

```python
# CloudWatch ë¹„ìš© ì¶”ì •
import boto3

cloudwatch = boto3.client('cloudwatch')

# ì¼ì¼ ë¹„ìš© ì¶”ì •
def estimate_daily_cost():
    # BytesInPerSec + BytesOutPerSec
    response = cloudwatch.get_metric_statistics(
        Namespace='AWS/Kafka',
        MetricName='BytesInPerSec',
        StartTime=datetime.utcnow() - timedelta(days=1),
        EndTime=datetime.utcnow(),
        Period=3600,
        Statistics=['Sum']
    )
    
    total_bytes = sum(point['Sum'] for point in response['Datapoints'])
    total_gb = total_bytes / (1024 ** 3)
    
    # ì½ê¸° + ì“°ê¸° ë¹„ìš©
    cost = total_gb * 0.10 * 2  # ì½ê¸°ì™€ ì“°ê¸°
    
    return cost
```

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. ì—°ê²° ë¬¸ì œ

#### ì¦ìƒ: "Connection refused" ë˜ëŠ” "Timeout"

**ì›ì¸ ë° í•´ê²°:**

```python
# 1. Bootstrap ì„œë²„ í™•ì¸
bootstrap_servers = "b-1.my-cluster.xxxxx.c2.kafka-serverless.ap-northeast-2.amazonaws.com:9098"

# 2. VPC ë° ë³´ì•ˆ ê·¸ë£¹ í™•ì¸
# - ì• í”Œë¦¬ì¼€ì´ì…˜ì´ MSKì™€ ê°™ì€ VPCì— ìˆëŠ”ì§€ í™•ì¸
# - ë³´ì•ˆ ê·¸ë£¹ì´ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸

# 3. DNS í•´ì„ í™•ì¸
import socket
host, port = bootstrap_servers.split(":")
try:
    ip = socket.gethostbyname(host)
    print(f"DNS resolved: {host} -> {ip}")
except socket.gaierror as e:
    print(f"DNS resolution failed: {e}")
```

### 2. ì¸ì¦ ë¬¸ì œ

#### ì¦ìƒ: "Authentication failed"

**ì›ì¸ ë° í•´ê²°:**

```python
# 1. IAM ì—­í• /ì •ì±… í™•ì¸
# - ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì˜¬ë°”ë¥¸ IAM ì—­í• ì„ ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸
# - MSK í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•œ ê¶Œí•œì´ ìˆëŠ”ì§€ í™•ì¸

# 2. ë¦¬ì „ í™•ì¸
# - í´ëŸ¬ìŠ¤í„° ë¦¬ì „ê³¼ í´ë¼ì´ì–¸íŠ¸ ë¦¬ì „ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸

# 3. ì¸ì¦ ë©”ì»¤ë‹ˆì¦˜ í™•ì¸
sasl_mechanism = "AWS_MSK_IAM"  # ì˜¬ë°”ë¥¸ ë©”ì»¤ë‹ˆì¦˜ ì‚¬ìš©
```

### 3. Consumer Lag ë¬¸ì œ

#### ì¦ìƒ: Consumerê°€ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ì§€ ëª»í•¨

**ì›ì¸ ë° í•´ê²°:**

```python
# 1. Consumer ê·¸ë£¹ ìƒíƒœ í™•ì¸
from aiokafka import AIOKafkaConsumer

async def check_consumer_lag(consumer: AIOKafkaConsumer):
    """Consumer lag í™•ì¸"""
    partitions = consumer.assignment()
    
    for partition in partitions:
        committed = await consumer.committed(partition)
        high_water = await consumer.highwater(partition)
        
        lag = high_water - committed
        print(f"Partition {partition}: Lag = {lag}")
        
        if lag > 10000:
            print(f"WARNING: High lag on {partition}")

# 2. ì²˜ë¦¬ ì†ë„ ê°œì„ 
# - ë³‘ë ¬ ì²˜ë¦¬
# - ë°°ì¹˜ ì²˜ë¦¬
# - íŒŒí‹°ì…˜ ìˆ˜ ì¦ê°€
```

### 4. ë©”ì‹œì§€ ì†ì‹¤ ë¬¸ì œ

#### ì¦ìƒ: ë°œí–‰í•œ ë©”ì‹œì§€ê°€ ì†Œë¹„ë˜ì§€ ì•ŠìŒ

**ì›ì¸ ë° í•´ê²°:**

```python
# 1. Producer ì„¤ì • í™•ì¸
producer = AIOKafkaProducer(
    acks="all",  # ëª¨ë“  ë³µì œë³¸ì— ì“°ê¸° ì™„ë£Œ ëŒ€ê¸°
    retries=3,  # ì¬ì‹œë„ í™œì„±í™”
    enable_idempotence=True,  # ë©±ë“±ì„± í™œì„±í™”
)

# 2. Consumer offset í™•ì¸
# - auto_offset_reset="earliest"ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë“  ë©”ì‹œì§€ ì½ê¸°
# - ìˆ˜ë™ ì»¤ë°‹ ì‚¬ìš©
```

### 5. ì„±ëŠ¥ ë¬¸ì œ

#### ì¦ìƒ: ì²˜ë¦¬ëŸ‰ì´ ë‚®ìŒ

**ì›ì¸ ë° í•´ê²°:**

```python
# 1. ë°°ì¹˜ í¬ê¸° ì¡°ì •
producer = AIOKafkaProducer(
    max_batch_size=32768,  # 32KBë¡œ ì¦ê°€
    linger_ms=50,  # ë°°ì¹˜ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
)

# 2. ì••ì¶• í™œì„±í™”
producer = AIOKafkaProducer(
    compression_type="gzip",
)

# 3. íŒŒí‹°ì…˜ ìˆ˜ ì¦ê°€
# - í† í”½ì˜ íŒŒí‹°ì…˜ ìˆ˜ë¥¼ ëŠ˜ë ¤ ë³‘ë ¬ ì²˜ë¦¬ ì¦ê°€
```

---

## ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 1. ë©”ì‹œì§€ ì„¤ê³„

#### 1.1 ìŠ¤í‚¤ë§ˆ ë²„ì „ ê´€ë¦¬

```python
# ë©”ì‹œì§€ì— ë²„ì „ í¬í•¨
@dataclass
class OrderCreatedEvent(DomainEvent):
    schema_version: str = "1.0"
    # ...
    
    def to_dict(self) -> dict:
        return {
            "schema_version": self.schema_version,
            # ...
        }
```

#### 1.2 ë©±ë“±ì„± ë³´ì¥

```python
# ì´ë²¤íŠ¸ IDë¥¼ ì‚¬ìš©í•œ ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€
processed_events = set()

async def handle_event(event_data: dict):
    event_id = event_data["event_id"]
    
    if event_id in processed_events:
        logger.warning(f"Duplicate event: {event_id}")
        return
    
    processed_events.add(event_id)
    # ì´ë²¤íŠ¸ ì²˜ë¦¬
```

### 2. ì—ëŸ¬ ì²˜ë¦¬

#### 2.1 Dead Letter Queue

```python
# ì‹¤íŒ¨í•œ ë©”ì‹œì§€ë¥¼ DLQë¡œ ì „ì†¡
async def handle_message_with_dlq(message, handler):
    try:
        await handler(message.value)
    except Exception as e:
        logger.error(f"Failed to process message: {e}")
        
        # DLQë¡œ ì „ì†¡
        await dlq_producer.send(
            topic="dlq.order.order.created",
            key=message.key,
            value={
                "original_message": message.value,
                "error": str(e),
                "failed_at": datetime.utcnow().isoformat(),
            }
        )
```

#### 2.2 ì¬ì‹œë„ ì „ëµ

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
async def process_with_retry(event_data: dict):
    """ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„"""
    await handle_event(event_data)
```

### 3. ë³´ì•ˆ

#### 3.1 ìµœì†Œ ê¶Œí•œ ì›ì¹™

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "kafka-cluster:Connect"
      ],
      "Resource": "arn:aws:kafka:*:*:cluster/*/*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "kafka-cluster:WriteData"
      ],
      "Resource": "arn:aws:kafka:*:*:topic/my-cluster/specific-topic/*"
    }
  ]
}
```

#### 3.2 ì•”í˜¸í™”

```python
# ì „ì†¡ ì¤‘ ì•”í˜¸í™” (TLS)
producer = AIOKafkaProducer(
    security_protocol="SASL_SSL",
    # ...
)

# ì €ì¥ ì‹œ ì•”í˜¸í™” (MSK Serverlessê°€ ìë™ ì²˜ë¦¬)
```

### 4. ëª¨ë‹ˆí„°ë§

#### 4.1 í•µì‹¬ ë©”íŠ¸ë¦­ ì¶”ì 

- Consumer Lag
- ë©”ì‹œì§€ ì²˜ë¦¬ ì†ë„
- ì—ëŸ¬ìœ¨
- ì²˜ë¦¬ ì§€ì—° ì‹œê°„

#### 4.2 ë¡œê¹…

```python
# êµ¬ì¡°í™”ëœ ë¡œê¹…
logger.info(
    "Message published",
    extra={
        "topic": topic,
        "partition": partition,
        "offset": offset,
        "event_type": event_type,
        "event_id": event_id,
    }
)
```

### 5. í…ŒìŠ¤íŠ¸

#### 5.1 ë¡œì»¬ í…ŒìŠ¤íŠ¸

```python
# ë¡œì»¬ Kafka ì‚¬ìš© (Docker)
# docker-compose.yml
version: '3'
services:
  kafka:
    image: confluentinc/cp-kafka:latest
    ports:
      - "9092:9092"
    environment:
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
```

#### 5.2 í†µí•© í…ŒìŠ¤íŠ¸

```python
# pytestë¥¼ ì‚¬ìš©í•œ í†µí•© í…ŒìŠ¤íŠ¸
@pytest.mark.asyncio
async def test_producer_consumer():
    producer = await create_test_producer()
    consumer = await create_test_consumer()
    
    # ë©”ì‹œì§€ ë°œí–‰
    await producer.publish("test-topic", "key", {"test": "data"})
    
    # ë©”ì‹œì§€ ì†Œë¹„
    message = await consumer.get_message(timeout=5)
    assert message.value["test"] == "data"
```

---

## ê²°ë¡ 

AWS MSK ServerlessëŠ” ì„œë²„ ê´€ë¦¬ ì—†ì´ Kafkaë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.
ë³¸ ê°€ì´ë“œì—ì„œ ë‹¤ë£¬ ë‚´ìš©ì„ ìš”ì•½í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

### í•µì‹¬ í¬ì¸íŠ¸

1. **ìë™ í™•ì¥**: íŠ¸ë˜í”½ì— ë”°ë¼ ìë™ìœ¼ë¡œ ìš©ëŸ‰ì´ ì¡°ì •ë˜ì–´ ìš©ëŸ‰ ê³„íšì´ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤.
2. **ë¹„ìš© íš¨ìœ¨**: ì‚¬ìš©í•œ ë§Œí¼ë§Œ ì§€ë¶ˆí•˜ëŠ” Pay-as-you-go ëª¨ë¸ë¡œ ì´ˆê¸° ë¹„ìš© ë¶€ë‹´ì´ ì ìŠµë‹ˆë‹¤.
3. **ë³´ì•ˆ**: IAM ê¸°ë°˜ ì¸ì¦ê³¼ VPC ê²©ë¦¬ë¡œ ì•ˆì „í•œ ë©”ì‹œì§• í™˜ê²½ì„ ì œê³µí•©ë‹ˆë‹¤.
4. **ê´€ë¦¬ í¸ì˜ì„±**: ë¸Œë¡œì»¤ ë° Zookeeper ê´€ë¦¬ê°€ í•„ìš” ì—†ì–´ ìš´ì˜ ë¶€ë‹´ì´ ì ìŠµë‹ˆë‹¤.

### êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### í´ëŸ¬ìŠ¤í„° ì„¤ì •
```
â–¡ VPC ë° ì„œë¸Œë„· êµ¬ì„± ì™„ë£Œ
â–¡ ë³´ì•ˆ ê·¸ë£¹ ì„¤ì • ì™„ë£Œ
â–¡ MSK Serverless í´ëŸ¬ìŠ¤í„° ìƒì„± ì™„ë£Œ
â–¡ IAM ì—­í•  ë° ì •ì±… ì„¤ì • ì™„ë£Œ
â–¡ Bootstrap ì„œë²„ ì£¼ì†Œ í™•ì¸ ì™„ë£Œ
```

#### Producer êµ¬í˜„
```
â–¡ IAM ì¸ì¦ ì„¤ì • ì™„ë£Œ
â–¡ Producer ì´ˆê¸°í™” ë° ì‹œì‘ ë¡œì§ êµ¬í˜„ ì™„ë£Œ
â–¡ ì´ë²¤íŠ¸ ë°œí–‰ ë¡œì§ êµ¬í˜„ ì™„ë£Œ
â–¡ ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§ êµ¬í˜„ ì™„ë£Œ
â–¡ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ êµ¬í˜„ ì™„ë£Œ
```

#### Consumer êµ¬í˜„
```
â–¡ Consumer ê·¸ë£¹ ì„¤ì • ì™„ë£Œ
â–¡ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ë“±ë¡ ì™„ë£Œ
â–¡ ë©”ì‹œì§€ ì†Œë¹„ ë£¨í”„ êµ¬í˜„ ì™„ë£Œ
â–¡ ìˆ˜ë™ ì»¤ë°‹ ë¡œì§ êµ¬í˜„ ì™„ë£Œ
â–¡ Dead Letter Queue ì²˜ë¦¬ êµ¬í˜„ ì™„ë£Œ
```

#### ëª¨ë‹ˆí„°ë§ ë° ìš´ì˜
```
â–¡ CloudWatch ëŒ€ì‹œë³´ë“œ êµ¬ì„± ì™„ë£Œ
â–¡ ì•ŒëŒ ì„¤ì • ì™„ë£Œ
â–¡ ë¡œê¹… ì „ëµ ìˆ˜ë¦½ ì™„ë£Œ
â–¡ ë¹„ìš© ëª¨ë‹ˆí„°ë§ ì„¤ì • ì™„ë£Œ
â–¡ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ ë¬¸ì„œí™” ì™„ë£Œ
```

### ë‹¤ìŒ ë‹¨ê³„

1. **ë¡œì»¬ ê°œë°œ í™˜ê²½ êµ¬ì¶•**: Docker Composeë¥¼ ì‚¬ìš©í•œ ë¡œì»¬ Kafka í™˜ê²½ êµ¬ì„±
2. **í†µí•© í…ŒìŠ¤íŠ¸ ì‘ì„±**: Producer/Consumer í†µí•© í…ŒìŠ¤íŠ¸ ì‘ì„±
3. **ì„±ëŠ¥ í…ŒìŠ¤íŠ¸**: ë¶€í•˜ í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ ì²˜ë¦¬ëŸ‰ ë° ì§€ì—° ì‹œê°„ ì¸¡ì •
4. **ìš´ì˜ ë¬¸ì„œí™”**: ìš´ì˜ ë§¤ë‰´ì–¼ ë° ì¥ì•  ëŒ€ì‘ ê°€ì´ë“œ ì‘ì„±
5. **ë¹„ìš© ìµœì í™”**: ì‹¤ì œ ì‚¬ìš©ëŸ‰ ë¶„ì„ì„ í†µí•œ ë¹„ìš© ìµœì í™”

### ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- [AWS MSK Serverless ê³µì‹ ë¬¸ì„œ](https://docs.aws.amazon.com/msk/latest/developerguide/serverless.html)
- [Apache Kafka ê³µì‹ ë¬¸ì„œ](https://kafka.apache.org/documentation/)
- [aiokafka ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¬¸ì„œ](https://aiokafka.readthedocs.io/)
- [AWS MSK Best Practices](https://docs.aws.amazon.com/msk/latest/developerguide/best-practices.html)
- [Kafka ë„¤ì´ë° ì»¨ë²¤ì…˜ ê°€ì´ë“œ](https://www.confluent.io/blog/kafka-topic-naming-conventions/)

### ì£¼ì˜ì‚¬í•­

1. **ë¦¬ì „ ì œí•œ**: MSK ServerlessëŠ” íŠ¹ì • ë¦¬ì „ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥í•˜ë¯€ë¡œ ë°°í¬ ë¦¬ì „ì„ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.
2. **íŒŒí‹°ì…˜ ì œí•œ**: í† í”½ë‹¹ ìµœëŒ€ 1,000ê°œ íŒŒí‹°ì…˜ ì œí•œì´ ìˆìœ¼ë¯€ë¡œ í† í”½ ì„¤ê³„ ì‹œ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.
3. **ë©”ì‹œì§€ í¬ê¸°**: ìµœëŒ€ 1MB ë©”ì‹œì§€ í¬ê¸° ì œí•œì´ ìˆìœ¼ë¯€ë¡œ ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” S3ì— ì €ì¥í•˜ê³  ì°¸ì¡°ë§Œ ì „ë‹¬í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
4. **ë¹„ìš© ëª¨ë‹ˆí„°ë§**: ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ê³¼ê¸ˆì´ë¯€ë¡œ CloudWatchë¥¼ í†µí•´ ì§€ì†ì ìœ¼ë¡œ ë¹„ìš©ì„ ëª¨ë‹ˆí„°ë§í•´ì•¼ í•©ë‹ˆë‹¤.

---

## ë¶€ë¡

### A. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì˜ˆì‹œ

```bash
# .env íŒŒì¼
KAFKA_BOOTSTRAP_SERVERS=b-1.my-msk-cluster.xxxxx.c2.kafka-serverless.ap-northeast-2.amazonaws.com:9098
KAFKA_REGION=ap-northeast-2
KAFKA_SECURITY_PROTOCOL=SASL_SSL
KAFKA_SASL_MECHANISM=AWS_MSK_IAM
KAFKA_CONSUMER_GROUP_ID=my-service-consumer
KAFKA_AUTO_OFFSET_RESET=earliest
KAFKA_ENABLE_AUTO_COMMIT=false
```

### B. Docker Compose ë¡œì»¬ ê°œë°œ í™˜ê²½

```yaml
# docker-compose.yml
version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
```

### C. ìœ ìš©í•œ AWS CLI ëª…ë ¹ì–´

```bash
# í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
aws kafka describe-cluster \
  --cluster-arn $CLUSTER_ARN \
  --region ap-northeast-2

# í† í”½ ëª©ë¡ ì¡°íšŒ
aws kafka list-topics \
  --cluster-arn $CLUSTER_ARN \
  --region ap-northeast-2

# Consumer ê·¸ë£¹ ëª©ë¡ ì¡°íšŒ
aws kafka list-consumer-groups \
  --cluster-arn $CLUSTER_ARN \
  --region ap-northeast-2

# Consumer ê·¸ë£¹ ìƒì„¸ ì •ë³´
aws kafka describe-consumer-group \
  --cluster-arn $CLUSTER_ARN \
  --consumer-group-id $GROUP_ID \
  --region ap-northeast-2

# í´ëŸ¬ìŠ¤í„° ì‚­ì œ
aws kafka delete-cluster \
  --cluster-arn $CLUSTER_ARN \
  --region ap-northeast-2
```

### D. ì—ëŸ¬ ì½”ë“œ ì°¸ì¡°

| ì—ëŸ¬ ì½”ë“œ | ì„¤ëª… | í•´ê²° ë°©ë²• |
|----------|------|----------|
| `ConnectionRefusedError` | ì—°ê²° ê±°ë¶€ | VPC ë° ë³´ì•ˆ ê·¸ë£¹ ì„¤ì • í™•ì¸ |
| `AuthenticationFailed` | ì¸ì¦ ì‹¤íŒ¨ | IAM ì—­í•  ë° ì •ì±… í™•ì¸ |
| `TopicAuthorizationException` | í† í”½ ê¶Œí•œ ì—†ìŒ | IAM ì •ì±…ì— í† í”½ ê¶Œí•œ ì¶”ê°€ |
| `ConsumerCoordinatorNotAvailableException` | Consumer ê·¸ë£¹ ì¡°ì •ì ì—†ìŒ | Consumer ê·¸ë£¹ ì¬ì‹œì‘ |
| `OffsetOutOfRangeException` | Offset ë²”ìœ„ ì´ˆê³¼ | `auto_offset_reset` ì„¤ì • í™•ì¸ |

### E. ì„±ëŠ¥ íŠœë‹ ê°€ì´ë“œ

#### Producer ì„±ëŠ¥ ìµœì í™”

```python
# ê³ ì„±ëŠ¥ Producer ì„¤ì •
producer = AIOKafkaProducer(
    bootstrap_servers=bootstrap_servers,
    # ë°°ì¹˜ ì„¤ì •
    max_batch_size=32768,  # 32KB
    linger_ms=50,  # ë°°ì¹˜ ëŒ€ê¸° ì‹œê°„
    # ì••ì¶•
    compression_type="gzip",
    # ì¬ì‹œë„
    retries=3,
    request_timeout_ms=30000,
    # ë²„í¼
    buffer_memory=67108864,  # 64MB
)
```

#### Consumer ì„±ëŠ¥ ìµœì í™”

```python
# ê³ ì„±ëŠ¥ Consumer ì„¤ì •
consumer = AIOKafkaConsumer(
    bootstrap_servers=bootstrap_servers,
    # ë°°ì¹˜ ì²˜ë¦¬
    max_poll_records=500,  # í•œ ë²ˆì— ê°€ì ¸ì˜¬ ìµœëŒ€ ë ˆì½”ë“œ ìˆ˜
    # ì„¸ì…˜ ê´€ë¦¬
    session_timeout_ms=30000,
    heartbeat_interval_ms=3000,
    # Fetch ì„¤ì •
    fetch_min_bytes=1048576,  # 1MB
    fetch_max_wait_ms=500,  # ìµœëŒ€ ëŒ€ê¸° ì‹œê°„
)
```

---

**ë¬¸ì„œ ë²„ì „**: 1.0  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-01-XX  
**ì‘ì„±ì**: Development Team 